{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import pprint as pp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "import configparser \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "ip = config['DEFAULT']['IP']\n",
    "port = config['DEFAULT']['MongoDB-Port']\n",
    "contain_string = config['DEFAULT']['Contain-String']\n",
    "\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(ip, int(port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twitter-richard-2018',\n",
      " 'twitter-richard-2015',\n",
      " '2019_W04_Twitter_Other',\n",
      " '2019_W08_Twitter_Other',\n",
      " '2019_W08_Twitter_Australia',\n",
      " 'twitter-richard-2017',\n",
      " '2019_W03_Twitter_Australia',\n",
      " '2019_W06_Twitter_Australia',\n",
      " '2018_W51_Twitter_Australia',\n",
      " '2019_W05_Twitter_Australia',\n",
      " '2019_W10_Twitter_Australia',\n",
      " '2019_W13_Twitter_Other',\n",
      " '2019_W12_Twitter_Other',\n",
      " '2019_W10_Twitter_Other',\n",
      " '2019_W14_Twitter_Other',\n",
      " 'twitter-richard-2016',\n",
      " '2019_W06_Twitter_Other',\n",
      " '2019_W01_Twitter_Other',\n",
      " 'twitter-richard-2014',\n",
      " '2019_W05_Twitter_Other',\n",
      " '2018_W52_Twitter_Other',\n",
      " '2019_W07_Twitter_Other',\n",
      " '2019_W12_Twitter_Australia',\n",
      " '2019_W07_Twitter_Australia',\n",
      " '2019_W01_Twitter_Australia',\n",
      " '2019_W02_Twitter_Australia',\n",
      " '2019_W02_Twitter_Other',\n",
      " '2019_W13_Twitter_Australia',\n",
      " '2019_W09_Twitter_Australia',\n",
      " '2018_W52_Twitter_Australia',\n",
      " '2019_W_Twitter_Other',\n",
      " '2018_W51_Twitter_Other',\n",
      " '2019_W14_Twitter_Australia',\n",
      " '2019_W11_Twitter_Australia',\n",
      " '2019_W03_Twitter_Other',\n",
      " '2019_W04_Twitter_Australia',\n",
      " '2019_W09_Twitter_Other',\n",
      " '2019_W11_Twitter_Other']\n"
     ]
    }
   ],
   "source": [
    "db_twitter = client[\"Twitter\"]\n",
    "collections_twitter = db_twitter.collection_names()\n",
    "pp.pprint(collections_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current year : 2019\n",
      "current week : 14\n"
     ]
    }
   ],
   "source": [
    "# get current year\n",
    "current_timestamp = int(time.time() * 1000)\n",
    "current_year = int(datetime.datetime.now().year)\n",
    "print(\"current year : \" + str(current_year))\n",
    "\n",
    "# get current week\n",
    "current_week = int((current_timestamp - 1546214400000)/1000/604800)+1\n",
    "print(\"current week : \" + str(current_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018_W51_Twitter_Australia: 20471\n",
      "2018_W51_Twitter_Other: 8978243\n",
      "2018_W52_Twitter_Australia: 38065\n",
      "2018_W52_Twitter_Other: 16940708\n",
      "2019_W01_Twitter_Australia: 40880\n",
      "2019_W01_Twitter_Other: 16994959\n",
      "2019_W02_Twitter_Australia: 37645\n",
      "2019_W02_Twitter_Other: 14474627\n",
      "2019_W03_Twitter_Australia: 52348\n",
      "2019_W03_Twitter_Other: 20416992\n",
      "2019_W04_Twitter_Australia: 59625\n",
      "2019_W04_Twitter_Other: 23417927\n",
      "2019_W05_Twitter_Australia: 61617\n",
      "2019_W05_Twitter_Other: 23917345\n",
      "2019_W06_Twitter_Australia: 61435\n",
      "2019_W06_Twitter_Other: 24176122\n",
      "2019_W07_Twitter_Australia: 28017\n",
      "2019_W07_Twitter_Other: 10484648\n",
      "2019_W08_Twitter_Australia: 60035\n",
      "2019_W08_Twitter_Other: 23446630\n",
      "2019_W09_Twitter_Australia: 58316\n",
      "2019_W09_Twitter_Other: 22293795\n",
      "2019_W10_Twitter_Australia: 40767\n",
      "2019_W10_Twitter_Other: 15941350\n",
      "2019_W11_Twitter_Australia: 68593\n",
      "2019_W11_Twitter_Other: 25345811\n",
      "2019_W12_Twitter_Australia: 50447\n",
      "2019_W12_Twitter_Other: 18090490\n",
      "2019_W13_Twitter_Australia: 62926\n",
      "2019_W13_Twitter_Other: 24379236\n"
     ]
    }
   ],
   "source": [
    "dic_collection = {}\n",
    "for i in collections_twitter:\n",
    "    if i.startswith(\"20\") and contain_string in i:\n",
    "        year = i[0:4]\n",
    "        week = re.search('_(.+?)_', i).group(1)[1:]\n",
    "        if int(year) < current_year:\n",
    "            dic_collection[i] = \"{:}\".format(db_twitter[i].find({}).count())\n",
    "        else:\n",
    "            try:\n",
    "                if int(week) < current_week:\n",
    "                    dic_collection[i] = \"{:}\".format(db_twitter[i].find({}).count())\n",
    "            except: pass\n",
    "\n",
    "for key in sorted(dic_collection):\n",
    "    print(\"%s: %s\" % (key, dic_collection[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a hashtag_city report from individual collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AU Cities: 220'\n",
      "'International Cities 22815'\n"
     ]
    }
   ],
   "source": [
    "# read city, state, country from csv file\n",
    "colnames = ['city', 'state', 'country']\n",
    "au_data = pandas.read_csv('supporting-files/au.csv', names=colnames, encoding=\"ISO-8859-1\")\n",
    "world_data = pandas.read_csv('supporting-files/world-cities.csv', names=colnames, encoding=\"ISO-8859-1\")\n",
    "pp.pprint ('AU Cities: {}'.format(len(au_data)))\n",
    "pp.pprint ('International Cities {}'.format(len(world_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\"$match\": { \"entities.hashtags\": {\"$exists\":True,\"$ne\":[]}}},\n",
    "    {\"$match\": { \"lang\" : \"en\"}},\n",
    "    { \"$group\": {\n",
    "        \"_id\": {\n",
    "            \"hashtags\": \"$entities.hashtags\",\n",
    "            \"location\": \"$user.location\"},\n",
    "        \"count\": { \"$sum\": 1 },\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list from csv file\n",
    "def get_list_from_csv(data):\n",
    "    ci = data.city.tolist()\n",
    "    city = ci[1:]\n",
    "    st = data.state.tolist()\n",
    "    state = st[1:]\n",
    "    co = data.country.tolist()\n",
    "    country = co[1:]\n",
    "    return city,state,country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create foler if not exist\n",
    "def create_folder():\n",
    "    folder = \"output/hashtag_city/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existed collection from the list dic_collection\n",
    "def delete_collection(folder,dic_collection):\n",
    "    for input_file in glob.glob(os.path.join(folder,'*.csv')):\n",
    "        collection_name = re.search('{(.+?)}', input_file).group(1)\n",
    "        print(\"Existed collection: \" + collection_name)\n",
    "        del dic_collection[collection_name]\n",
    "    return dic_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to add count number\n",
    "def count(dc,data,exist,c):\n",
    "    dc += data[\"count\"]\n",
    "    exist = 1\n",
    "    c+=1\n",
    "    return dc,exist,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to dictionary\n",
    "def append_dic(data_format,exist,c,dic):\n",
    "    data_format.append(dic)\n",
    "    exist = 1\n",
    "    c+=1\n",
    "    return data_format,exist,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if exists then add count number\n",
    "def check_count(data_format, x,text,s_geo,exist,c,data):\n",
    "    for d in data_format:\n",
    "        s0 = d[\"id\"].split(\",\")[0]\n",
    "        s1 = s = d[\"id\"].split(\",\")[1]\n",
    "        if(x in d[\"id\"]) and (text in d[\"hashtag\"]) and (s_geo == \"city\"):\n",
    "            d[\"count\"],exist,c = count(d[\"count\"],data,exist,c)\n",
    "            break\n",
    "        elif (s0 == \"null\") and (x in d[\"id\"]) and (text in d[\"hashtag\"]) and (s_geo == \"state\"):\n",
    "            d[\"count\"],exist,c = count(d[\"count\"],data,exist,c)\n",
    "            break\n",
    "        elif (s1 == \"null\") and (x in d[\"id\"]) and (text in d[\"hashtag\"])and (s_geo == \"country\"):\n",
    "            d[\"count\"],exist,c = count(d[\"count\"],data,exist,c)\n",
    "            break\n",
    "    return data_format,exist,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate running time\n",
    "def calculate_time(start_time, t):\n",
    "    current_time = time.time()\n",
    "    duration = current_time - start_time\n",
    "    if (duration/3600) >= (t+1):\n",
    "        t += 1\n",
    "        print(\"The program is still running, already run for about \"+ str(t) + \" hours.\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for each loop\n",
    "def floop(s_geo,geo,data,data_format,text,state,country,exist,c,start_time,t):\n",
    "    for x in geo: \n",
    "        if isinstance(x,str):\n",
    "            if s_geo == \"city\":\n",
    "                loc = data[\"_id\"][\"location\"].lower().replace(\"australia\",\"\")\n",
    "            else:\n",
    "                loc = data[\"_id\"][\"location\"].lower()\n",
    "            if x.lower() in loc:\n",
    "                index = geo.index(x)\n",
    "    \n",
    "                if s_geo == \"city\":\n",
    "                    sid = x + ',' + str(state[index]) + \",\" + str(country[index])\n",
    "                if s_geo == \"state\":\n",
    "                    sid = \"null\" + \",\" + x + \",\" + str(country[index])\n",
    "                if s_geo == \"country\":\n",
    "                    sid = \"null\" + \",\" + \"null\" + ',' + x\n",
    "                dic = {\"id\":sid,\"hashtag\":text,\"count\":data[\"count\"]}\n",
    "                \n",
    "                if len(data_format)>0:\n",
    "                    data_format,exist,c = check_count(data_format, x,text,s_geo,exist,c,data)\n",
    "                    if exist == 0:\n",
    "                        data_format,exist,c = append_dic(data_format,exist,c,dic)\n",
    "                else: \n",
    "                    data_format,exist,c = append_dic(data_format,exist,c,dic)\n",
    "                    \n",
    "                #print(\"No.\" + str(c) + s_geo + \": \" + x)\n",
    "                \n",
    "                # print every hour if it's still running\n",
    "                t = calculate_time(start_time, t)\n",
    "                break\n",
    "   \n",
    "    return data_format,exist,c,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv file\n",
    "def write_csv(collection,total_tweet,tweets_htag,data_format):\n",
    "    \n",
    "    file_name = \"output/hashtag_city/{\" + collection + \"}_hashtag_city.csv\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        # header\n",
    "        f.write('hashtag,city,state,country,number of tweets,tweets_htag,tweets_nohtag\\n')\n",
    "            \n",
    "        tweets_nohtag = int(total_tweet)-tweets_htag\n",
    "\n",
    "        for data in data_format:\n",
    "            city = data['id'].split(\",\")[0]\n",
    "            state = data['id'].split(\",\")[1]\n",
    "            country = data['id'].split(\",\")[2]\n",
    "            if \"Australia\" in collection:\n",
    "                if state == 'Victoria':\n",
    "                    state = 'VIC'\n",
    "\n",
    "            line = data[\"hashtag\"] + ',' + city + ',' + state + ',' +  country + ',' + \\\n",
    "            str(data['count']) + ','+ str(tweets_htag)+ ',' +str(tweets_nohtag)+ '\\n'\n",
    "            \n",
    "            f.write(line)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "\n",
      "Processing on collection: 2018_W51_Twitter_Australia\n",
      "Aggregating...\n",
      "Aggragation finished.\n",
      "hashtag list is finished\n",
      "csv file for collection 2018_W51_Twitter_Australia is finished.\n",
      "-----------------------\n",
      "\n",
      "-----------------------\n",
      "\n",
      "Processing on collection: 2018_W51_Twitter_Other\n",
      "Aggregating...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-dca7f87addd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aggregating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_twitter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallowDiskUse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aggragation finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, session, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                                    \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m                                    \u001b[0mexplicit_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m                                    **kwargs)\n\u001b[0m\u001b[1;32m   2398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate_raw_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, pipeline, cursor_class, first_batch_size, session, explicit_session, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m                 \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m                 client=self.__database.client)\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"cursor\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;31m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events)\u001b[0m\n\u001b[1;32m    577\u001b[0m                            \u001b[0mcompression_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                            \u001b[0muse_op_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_msg_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                            unacknowledged=unacknowledged)\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mresponse_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreceive_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0munpacked_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodec_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcodec_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(sock, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Ignore the response's request id.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     length, _, response_to, op_code = _UNPACK_HEADER(\n\u001b[0;32m--> 173\u001b[0;31m         _receive_data_on_socket(sock, 16))\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;31m# No request_id for exhaust cursor \"getMore\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36m_receive_data_on_socket\u001b[0;34m(sock, length)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbytes_read\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mchunk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_errno_from_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_columns = ['hashtag','city','state','country','count']\n",
    "\n",
    "#create folder if not exist\n",
    "folder = create_folder()\n",
    "    \n",
    "# delete existed collection from the list dic_collection\n",
    "dic_collection = delete_collection(folder,dic_collection)\n",
    "\n",
    "\n",
    "for collection in sorted(dic_collection):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    t = 0\n",
    " \n",
    "    total_tweet = dic_collection[collection]\n",
    "    if \"Australia\" in collection:     \n",
    "        city, state, country = get_list_from_csv(au_data)\n",
    "    elif \"Other\" in collection:\n",
    "        city, state, country = get_list_from_csv(world_data)\n",
    "\n",
    "    print(\"-----------------------\\n\")\n",
    "    print(\"Processing on collection: \" + collection)\n",
    "\n",
    "    data_format = []\n",
    "    \n",
    "    print(\"Aggregating...\")\n",
    "    data_list = list(db_twitter[collection].aggregate(pipeline,allowDiskUse=True))\n",
    "    print(\"Aggragation finished.\")\n",
    "    c = 0 \n",
    "    tweets_htag = 0\n",
    "    \n",
    "    if len(data_list) > 0 : \n",
    "        for data in data_list:\n",
    "            tweets_htag += data[\"count\"]\n",
    "            for htag in data[\"_id\"][\"hashtags\"]:\n",
    "                # get hashtag\n",
    "                text = htag[\"text\"].lower()\n",
    "                # check if it is in English\n",
    "                if(re.match(\"^[a-zA-Z0-9]*$\",text)):\n",
    "                    exist = 0\n",
    "                    # check if the location is null\n",
    "                    if data[\"_id\"][\"location\"] is not None: \n",
    "                        s_geo = \"city\"\n",
    "                        data_format,exist,c,t = floop(s_geo,city,data,data_format,text,state,country,exist,c,start_time,t)\n",
    "                        if exist == 0:\n",
    "                            s_geo = \"state\"\n",
    "                            data_format,exist,c,t = floop(s_geo,state,data,data_format,text,state,country,exist,c,start_time,t)\n",
    "                        if exist == 0:\n",
    "                            s_geo = \"country\"\n",
    "                            data_format,exist,c,t = floop(s_geo,country,data,data_format,text,state,country,exist,c,start_time,t)\n",
    "\n",
    "\n",
    "        print(\"hashtag list is finished\")\n",
    "\n",
    "        write_csv(collection,total_tweet,tweets_htag,data_format)\n",
    "\n",
    "        print (\"csv file for collection \" + collection + ' is finished.')\n",
    "        print(\"-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
